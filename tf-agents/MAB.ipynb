{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TenArmedTestbed(tfa.environments.py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._observation_spec = tfa.specs.array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=-4, maximum=4, name='observation')\n",
    "        self._action_spec = tfa.specs.array_spec.BoundedArraySpec(\n",
    "            shape=(10,), dtype=np.int32, minimum=0, maximum=9, name='action')\n",
    "        \n",
    "        self._action_values = np.random.normal(size=(10,))\n",
    "        \n",
    "        super(tfa.environments.py_environment.PyEnvironment, self).__init__()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        return ts.restart(0.0, batch_size=self.batch_size)\n",
    "\n",
    "    def _step(self, action):\n",
    "        observation = np.random.normal(self._action_values, size=(10,))[action]\n",
    "        reward = observation\n",
    "        return ts.termination(observation, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(tfa.policies.tf_policy.TFPolicy):\n",
    "    \n",
    "    def __init__(self, action_values):\n",
    "        observation_spec = tfa.specs.tensor_spec.BoundedTensorSpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=-4, maximum=4, name='observation')\n",
    "        time_step_spec = ts.time_step_spec(observation_spec)\n",
    "        \n",
    "        action_spec = tfa.specs.tensor_spec.BoundedTensorSpec(\n",
    "            shape=(10,), dtype=np.int32, minimum=0, maximum=9)\n",
    "        \n",
    "        self._action_values = action_values\n",
    "\n",
    "        super(GreedyPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                           action_spec=action_spec)\n",
    "    \n",
    "    def _action(self, time_step, policy_state, seed=0):\n",
    "        action = tf.argmax(self._action_values, output_type=tf.int32)\n",
    "        return tfa.trajectories.policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(tfa.policies.tf_policy.TFPolicy):\n",
    "    \n",
    "    def __init__(self, epsilon, action_values):\n",
    "        observation_spec = tfa.specs.tensor_spec.BoundedTensorSpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=-4, maximum=4, name='observation')\n",
    "        time_step_spec = ts.time_step_spec(observation_spec)\n",
    "        \n",
    "        action_spec = tfa.specs.tensor_spec.BoundedTensorSpec(\n",
    "            shape=(10,), dtype=np.int32, minimum=0, maximum=9)\n",
    "        \n",
    "        self._epsilon = epsilon\n",
    "        self._action_values = action_values\n",
    "\n",
    "        super(EpsilonGreedyPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                                  action_spec=action_spec)\n",
    "    \n",
    "    def _action(self, time_step, policy_state, seed=0):\n",
    "        \n",
    "        if tf.random.uniform([1], maxval=1) < self._epsilon:\n",
    "            action = tf.random.uniform([1], maxval=10, dtype=tf.int32)\n",
    "        else:\n",
    "            action = tf.argmax(self._action_values, output_type=tf.int32)\n",
    "        \n",
    "        return tfa.trajectories.policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tfa.agents.tf_agent.TFAgent):\n",
    "    \n",
    "    def __init__(self, epsilon, niter):\n",
    "        self.action_values = tf.Variable(np.zeros(10), dtype=tf.float32)\n",
    "        self.naction = tf.Variable(np.zeros(10), dtype=tf.float32)\n",
    "        self.history = [0]\n",
    "        self.nsteps = tf.Variable(0, dtype=tf.int32)\n",
    "        \n",
    "        epsilon = tf.constant(epsilon)\n",
    "        policy = GreedyPolicy(self.action_values)\n",
    "        collect_policy = EpsilonGreedyPolicy(epsilon, self.action_values)\n",
    "        \n",
    "        time_step_spec = policy.time_step_spec\n",
    "        action_spec = policy.action_spec\n",
    "        \n",
    "        super(Agent, self).__init__(time_step_spec=time_step_spec,\n",
    "                                    action_spec=action_spec,\n",
    "                                    policy=policy,\n",
    "                                    collect_policy=collect_policy,\n",
    "                                    train_sequence_length=None)\n",
    "    \n",
    "    def _train(self, experience, weights=None):\n",
    "        \n",
    "        observation = experience.observation\n",
    "        action = experience.action\n",
    "        reward = experience.reward\n",
    "        self.nsteps.assign_add(1)\n",
    "        \n",
    "        action_index = tf.reshape(action, (1,1))\n",
    "        step_index = tf.reshape(self.nsteps, (1,1))\n",
    "    \n",
    "        average_rewards = self.history[self.nsteps-1] + \\\n",
    "                          1/tf.cast(self.nsteps, tf.float32) * \\\n",
    "                          (reward - self.history[self.nsteps-1])\n",
    "        \n",
    "        self.history.append(tf.squeeze(average_rewards).numpy())\n",
    "        \n",
    "        self.naction.assign(\n",
    "            tf.tensor_scatter_nd_add(\n",
    "                self.naction,\n",
    "                action_index,\n",
    "                [1]))\n",
    "        \n",
    "        action_value_update = tf.reshape(1/self.naction[tf.squeeze(action)] * \\\n",
    "                                             (reward - self.action_values[tf.squeeze(action)]),\n",
    "                                        (1))\n",
    "        self.action_values.assign(\n",
    "            tf.tensor_scatter_nd_add(\n",
    "                self.action_values,\n",
    "                action_index,\n",
    "                action_value_update))\n",
    "        \n",
    "        return tfa.agents.tf_agent.LossInfo((), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "    return tfa.trajectories.trajectory.Trajectory(\n",
    "                                observation=tf.expand_dims(initial_step.observation, 0),\n",
    "                                action=tf.expand_dims(action_step.action, 0),\n",
    "                                policy_info=action_step.info,\n",
    "                                reward=tf.expand_dims(final_step.reward, 0),\n",
    "                                discount=tf.expand_dims(final_step.discount, 0),\n",
    "                                step_type=tf.expand_dims(initial_step.step_type, 0),\n",
    "                                next_step_type=tf.expand_dims(final_step.step_type, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "environment = TenArmedTestbed()\n",
    "tf_environment = tfa.environments.tf_py_environment.TFPyEnvironment(environment)\n",
    "\n",
    "niter = 1000\n",
    "agent = Agent(epsilon, niter)\n",
    "\n",
    "step = tf_environment.reset()\n",
    "for _ in range(niter):\n",
    "    action_step = agent.collect_policy.action(step)\n",
    "    next_step = tf_environment.step(action_step.action)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "    # print(experience)\n",
    "    agent.train(experience)\n",
    "    step = next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "history = np.zero\n",
    "for _ in range(100):\n",
    "    environment = TenArmedTestbed()\n",
    "    tf_environment = tfa.environments.tf_py_environment.TFPyEnvironment(environment)\n",
    "    agent = Agent(epsilon)\n",
    "\n",
    "    step = tf_environment.reset()\n",
    "    for __ in range(1000):\n",
    "        action_step = agent.collect_policy.action(step)\n",
    "        next_step = tf_environment.step(action_step.action)\n",
    "        experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "        # print(experience)\n",
    "        agent.train(experience)\n",
    "        step = next_step\n",
    "    history = history + np.trim_zeros(agent._history.numpy())\n",
    "history = history/100\n",
    "\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
